{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \"What is Gradient Descent?\"\n",
        "> \"Gradient descent is a commonly used optimization algorithm in machine learning, especially for regression problems.\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- categories: [gradient descent, optimization, MSE, RSS, partial derivatives]"
      ],
      "metadata": {
        "id": "Rhz7ty74HfeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* According to [Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent), \"**gradient descent** is a first-order iterative optimization algorithm for finding a *local minimum* of a differentiable function.\"\n",
        "  * ![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Extrema_example_original.svg/220px-Extrema_example_original.svg.png)\n",
        "* This is the algorithm's equation, which needs repeated until a local minimum is reached for each weight, from $j=0$ to $k$:\n",
        "  * $w_j = w_j - \\alpha\\frac{\\delta}{\\delta w_i}J(w_j,...,w_k)$\n",
        "* For a model with one weight and one bias:\n",
        "  * $w_0 = w_0 - \\alpha\\frac{\\delta}{\\delta w_0}J(w_0,w_1)$\n",
        "  * $w_1 = w_1 - \\alpha \\frac{\\delta}{\\delta w_1}J(w_0,w_1)$\n",
        "* For a model with two weights and one bias:\n",
        "  * $w_0 = w_0 - \\alpha  \\frac{\\delta}{\\delta w_0}J(w_0,w_1,w_2)$\n",
        "  * $w_1 = w_1 - \\alpha  \\frac{\\delta}{\\delta w_1}J(w_0,w_1,w_2)$\n",
        "  * $w_2 = w_2 - \\alpha  \\frac{\\delta}{\\delta w_2}J(w_0,w_1,w_2)$\n",
        "* For a model with $k-1$ weights and one bias:\n",
        "  * $w_0 = w_0 - \\alpha  \\frac{\\delta}{\\delta w_0}J(w_0,w_1,w_2,...,w_k)$\n",
        "  * $w_1 = w_1 - \\alpha  \\frac{\\delta}{\\delta w_1}J(w_0,w_1,w_2,...,w_k)$\n",
        "  * $w_2 = w_2 - \\alpha  \\frac{\\delta}{\\delta w_2}J(w_0,w_1,w_2,...,w_k)$\n",
        "  * ...\n",
        "  * $w_k = w_k - \\alpha \\frac{\\delta}{\\delta w_k}J(w_0,w_1,w_2,...,w_k)$\n",
        "\n",
        "\n",
        "## Components\n",
        "* $\\alpha$ is known as the step size or learning rate. It dictates how quickly a model will be optimized.\n",
        "* According to [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/), $J$ is a **cost function**, which \"quantifies the error between predicted values and expected values and presents it in the form of a single real number.\"\n",
        "* The **mean squared error (MSE)** is a commonly used cost function:\n",
        "  * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (\\hat{Y}_i - Y_i)^2$\n",
        "    * $n$ represents the sample size\n",
        "    * $\\hat{Y}_i$ represents the predicted value, which is output from a model\n",
        "    * $Y_i$ represents the expected value, which should be held constant during optimization\n",
        "\n",
        "\n",
        "## Rewriting the MSE\n",
        "* To best understand this cost function, $\\hat{Y}_i$ must be expressed using weights.\n",
        "  * For a model with one weight ($w_1$) and one bias ($w_0$):\n",
        "    * $\\hat{Y}_i = f(w_0,w_1) = w_1X_i+w_0$\n",
        "      * $X_i$ represents the features, which should be held constant during optimization\n",
        "      * This is a line.\n",
        "  * For a model with two weights ($w_1$ and $w_2$) and one bias ($w_0$): \n",
        "\n",
        "   * $\\hat{Y}_i = f(w_0,w_1,w_2) = w_{2} X_{2_i}+ w_1 X_{1_i} +w_0$\n",
        "\n",
        "      * $X_1$ and $X_2$ represent the features, which should be held constant during optimization\n",
        "      * This is a plane.\n",
        "* In terms of weights, the MSE can be represented:\n",
        "  * For one weight and one bias ($\\hat{Y}_i = f(w_0,w_1) = w_1X_i+w_0$):\n",
        "    * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (f(w_0,w_1) - Y_i)^2$\n",
        "    * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (w_1X_i+w_0 - Y_i)^2$\n",
        "  * For two weights and one bias ($\\hat{Y}_i = f(w_0,w_1,w_2) = w_2X_2+w_1X_1+w_0$):\n",
        "    * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (f(w_0,w_1,w_2) - Y_i)^2$\n",
        "    * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)^2$\n",
        "  * For $k-1$ weights and one bias ( $\\hat{Y}_i = f(w_0,w_1,w_2,...,w_k) = w_0 + \\sum\\limits^{k}_{j=1} w_jX_{j_i} = w_0 + w_1X_1 + ... + w_kX_k)$:\n",
        "    * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (f(w_0,w_1,w_2,...,w_k) - Y_i)^2$\n",
        "   * $MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (w_0 + \\sum\\limits^{k}_{j=1}w_jX_{j_i} - Y_i)^2$\n",
        "\n",
        "## MSE as a Cost Function \n",
        "* Now, $MSE$ can be expressed using $J$:\n",
        "  * For one weight and one bias:\n",
        "    * $MSE = J(w_0,w_1) = \\frac{1}{n}\\sum_{i=1}^{n} (w_1X_i+w_0 - Y_i)^2$\n",
        "  * For two weights and bias:\n",
        "    * $MSE = J(w_0,w_1,w_2) = \\frac{1}{n}\\sum_{i=1}^{n} (w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)^2$\n",
        "  * For $k$ weights and one bias:\n",
        "   * $MSE = J(w_0,w_1,w_2,...,w_k) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (w_0 + \\sum\\limits^{k}_{j=1}w_jX_{j_i} - Y_i)^2$\n",
        "* As a reminder, $X_{1...k}$ and $Y$ should be treated as constants during optimization.\n"
      ],
      "metadata": {
        "id": "b5AiJDCGGdRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Derivatives\n",
        "* For gradient descent, the partial derivative, $\\frac{\\delta}{\\delta w_j}J(w_j,...,w_k)$, with respect to $w_j$ for $j=0$ to $k$, must be found:\n",
        "  * $w_j = w_j - \\alpha * \\frac{\\delta}{\\delta w_j}J(w_j,...,w_k)$\n",
        "* Partial derivatives are used when a function is made up of multiple variables and when the goal is to see how the function changes when just one variable changes while holding the others constant.\n",
        "* The following summary from [Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) proves useful:\n",
        "  * <img src=\"https://drive.google.com/uc?export=view&id=1D3kKtnmxQjmhAuOTnYuP8C7fRPNUfSkq\" width=400>\n",
        "* These generic rules for derivatives are also useful:"
      ],
      "metadata": {
        "id": "3UIfl1YXUzT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1>\n",
        "from IPython.display import IFrame\n",
        "IFrame(\"https://drive.google.com/file/d/11sgPFPMGu9OUOM0g7KZ3JU735Wdlxqi5/preview\", width=800, height=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9rANxcqNfTvX",
        "outputId": "4bfdbb4d-e6d2-4978-b0de-6c372e3f0aab",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f3e500f1a30>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"300\"\n",
              "            src=\"https://drive.google.com/file/d/11sgPFPMGu9OUOM0g7KZ3JU735Wdlxqi5/preview\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent with One Weight and One Bias Using MSE as the Cost Function\n",
        "* For a model with one weight and one bias:\n",
        "  * $w_0 = w_0 - \\alpha * \\frac{\\delta}{\\delta w_0}J(w_0,w_1)$\n",
        "  * $w_1 = w_1 - \\alpha * \\frac{\\delta}{\\delta w_1}J(w_0,w_1)$\n",
        "* Partial derivatives have to be used to find $\\frac{\\delta}{\\delta w_0}J(w_0,w_1)$ and $\\frac{\\delta}{\\delta w_1}J(w_0,w_1)$:\n"
      ],
      "metadata": {
        "id": "T5svrTlsi28Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1>\n",
        "from IPython.display import IFrame\n",
        "IFrame(\"https://drive.google.com/file/d/1tVEKyL_pI_VEyK_EnNIzlbnp8phMrksP/preview\", width=800, height=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MxgshmQw8EoE",
        "outputId": "3cb96226-f32f-441e-ae95-5822af2b5173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f3e500f1820>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"300\"\n",
              "            src=\"https://drive.google.com/file/d/1tVEKyL_pI_VEyK_EnNIzlbnp8phMrksP/preview\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The new formulas for gradient descent will be as follows:\n",
        "  * $w_0 = w_0 - \\alpha * \\frac{2}{n}\\sum\\limits_{i=1}^{n}(w_1X_i+w_0 - Y_i)$\n",
        "  * $w_1 = w_1 - \\alpha * \\frac{2}{n}\\sum\\limits_{i=1}^{n}X_i(w_1X_i+w_0 - Y_i)$\n",
        "* It is worth mentioning that it is common to see the 2 dropped from the equation as a mathematical convenience.\n",
        "  * To do this, the original MSE function is rewritten as $MSE = J(w_0,w_1) = \\frac{1}{2n}\\sum_{i=1}^{n} (w_1X_i+w_0 - Y_i)^2$\n",
        "* When the derivative is computed, the 2 in the numerator and the 2 in the denominator cancel out, leaving the following equations:\n",
        "    * $w_0 = w_0 - \\alpha * \\frac{1}{n}\\sum\\limits_{i=1}^{n}(w_1X_i+w_0 - Y_i)$\n",
        "    * $w_1 = w_1 - \\alpha * \\frac{1}{n}\\sum\\limits_{i=1}^{n}X_i(w_1X_i+w_0 - Y_i)$\n",
        "* If this still doesn't make sense, the 2 in the numerator could be thought of as scaling the learning rate:\n",
        "  * Hence, dividing by 2 is the equivalent of making the learning rate half the size as it was originally.\n",
        "    * $w_0 = w_0 - \\frac{\\alpha}{2}\\frac{2}{n}\\sum\\limits_{i=1}^{n}(w_1X_i+w_0 - Y_i)$\n",
        "  * Regardless of the equation used, the minimum will be the same since the only visible difference would be the learning rate.\n",
        "    * Keeping the 2 would make it take more epochs for the minimum to be found.\n"
      ],
      "metadata": {
        "id": "Us3JvNf9AKIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Gradient Descent with Two Weights and One Bias Using MSE as the Cost Function\n",
        "* For a model with two weights and one bias:\n",
        "  * $w_0 = w_0 - \\alpha * \\frac{\\delta}{\\delta w_0}J(w_0,w_1,w_2)$\n",
        "  * $w_1 = w_1 - \\alpha * \\frac{\\delta}{\\delta w_1}J(w_0,w_1,w_2)$\n",
        "  * $w_2 = w_2 - \\alpha * \\frac{\\delta}{\\delta w_2}J(w_0,w_1,w_2)$\n",
        "* Partial derivatives have to be used to find $\\frac{\\delta}{\\delta w_0}J(w_0,w_1,w_2)$, $\\frac{\\delta}{\\delta w_1}J(w_0,w_1,w_2)$, and $\\frac{\\delta}{\\delta w_2}J(w_0,w_1,w_2)$:\n"
      ],
      "metadata": {
        "id": "wXo2ubH3nOEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1>\n",
        "from IPython.display import IFrame\n",
        "IFrame(\"https://drive.google.com/file/d/1HK2b7_l1hsYYOuykxZ_HCiCupXa4vQA4/preview\", width=800, height=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MUVpxy8ncAPi",
        "outputId": "e7ba8e2d-6ff0-45cd-cdc0-9ef7f2d1b8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f3e500f1eb0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"300\"\n",
              "            src=\"https://drive.google.com/file/d/1HK2b7_l1hsYYOuykxZ_HCiCupXa4vQA4/preview\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The new formulas for gradient descent are as follows:\n",
        "  * $w_0 = w_0 - \\alpha[\\frac{2}{n}\\sum\\limits_{i=1}^{n}(w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)]$ \n",
        "  * $w_1 = w_1 - \\alpha[\\frac{2}{n}\\sum\\limits_{i=1}^{n}X_{1_i}(w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)]$\n",
        "  * $w_2 = w_2 - \\alpha[\\frac{2}{n}\\sum\\limits_{i=1}^{n}X_{2_i}(w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)]$\n",
        "* Remember, if the MSE or learning rate are halfed, the 2 will be divided out of the equation, leaving the following equations: \n",
        "  * $w_0 = w_0 - \\alpha [\\frac{1}{n}\\sum\\limits_{i=1}^{n}(w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)]$ \n",
        "  * $w_1 = w_1 - \\alpha [\\frac{1}{n}\\sum\\limits_{i=1}^{n}X_{1_i}(w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)]$\n",
        "  * $w_2 = w_2 - \\alpha [\\frac{1}{n}\\sum\\limits_{i=1}^{n}X_{2_i}(w_2X_{2_i}+w_1X_{1_i}+w_0 - Y_i)]$"
      ],
      "metadata": {
        "id": "q2r-IIZ3chT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A General Formula\n",
        "* Let $\\hat{Y} = w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i}$ for $k$ features.\n",
        "* $MSE = \\frac{1}{n}\\sum_\\limits{i=1}^{n}(\\hat{Y}_i-Y_i)^2= \\frac{1}{n}\\sum_\\limits{i=1}^{n}((w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i})-Y_i)^2$\n",
        "* The partial derivatives of the weights could be represented as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\delta MSE}{\\delta w_j} = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        \\frac{1}{n}\\sum\\limits_{i=1}^{n}(\\hat{Y}_i -Y_i) & \\text{if } j = 0\\\\\n",
        "        \\frac{1}{n}\\sum\\limits_{i=1}^{n}X_{j_i}(\\hat{Y}_i -Y_i)& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "* For $j=0$ to $k$ features, gradient descent would be: $w_j = w_j - \\alpha \\frac{\\delta MSE}{\\delta w_j}$\n",
        "  * Alternatively:\n",
        "  \\begin{equation}\n",
        "w_j = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        w_0 - \\alpha\\frac{1}{n}\\sum\\limits_{i=1}^{n}(\\hat{Y}_i -Y_i) & \\text{if } j = 0\\\\\n",
        "        w_j - \\alpha\\frac{1}{n}\\sum\\limits_{i=1}^{n}X_{j_i}(\\hat{Y}_i -Y_i)& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}\n",
        "  * With the dot product:\n",
        "  \\begin{equation}\n",
        "w_j = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        w_0 - \\alpha\\frac{1}{n}\\sum\\limits_{i=1}^{n}(\\hat{Y}_i -Y_i) & \\text{if } j = 0\\\\\n",
        "        w_j - \\alpha\\frac{1}{n}[X_{j_i}\\cdot (\\hat{Y}_i -Y_i)]& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "royymBKEYC4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent with the Residual Sum of Squares (RSS)\n",
        "* The **residual sum of squares** is a cost function that is commonly used and nearly identical to the MSE, but it does not find the average:\n",
        "  * $RSS = \\sum\\limits_{i=1}^{n} (\\hat{Y}_i - Y_i)^2 = \\sum\\limits_{i=1}^{n} (w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i} - Y_i)^2$\n",
        "* RSS can be used as the cost function in gradient descent as well.\n",
        "  * Without the additional division by the number of samples, a smaller learning rate will have a larger impact in most cases.\n",
        "* The partial derivatives of the weights could be represented as:\n",
        "\\begin{equation}\n",
        "\\frac{\\delta RSS}{\\delta w_j} = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        2\\sum\\limits_{i=1}^{n}(w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i} -Y_i) & \\text{if } j = 0\\\\\n",
        "        2\\sum\\limits_{i=1}^{n}X_{j_i}(w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i} -Y_i)& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}\n",
        "* By using $\\text{One-Half RSS} = \\frac{1}{2}\\sum\\limits_{i=1}^{n} (\\hat{Y}_i - Y_i)^2 = \\frac{1}{2}\\sum\\limits_{i=1}^{n} (w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i} - Y_i)^2$:\n",
        "\\begin{equation}\n",
        "\\frac{\\delta RSS}{\\delta w_j} = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        \\sum\\limits_{i=1}^{n}(w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i} -Y_i) & \\text{if } j = 0\\\\\n",
        "        \\sum\\limits_{i=1}^{n}X_{j_i}(w_0 + \\sum\\limits_{j=1}^{k} w_jX_{j_i} -Y_i)& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "* For $j=0$ to $k$ features, gradient descent would be: $w_j = w_j - \\alpha \\frac{\\delta RSS}{\\delta w_j}$\n",
        "  * Alternatively:\n",
        "  \\begin{equation}\n",
        "w_j = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        w_0 - \\alpha\\sum\\limits_{i=1}^{n}(\\hat{Y}_i -Y_i) & \\text{if } j = 0\\\\\n",
        "        w_j - \\alpha\\sum\\limits_{i=1}^{n}X_{j_i}(\\hat{Y}_i -Y_i)& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}\n",
        "  * With the dot product:\n",
        "  \\begin{equation}\n",
        "w_j = \n",
        "\\left\\{\n",
        "    \\begin{array}{lr}\n",
        "        w_0 - \\alpha\\sum\\limits_{i=1}^{n}(\\hat{Y}_i -Y_i) & \\text{if } j = 0\\\\\n",
        "        w_j - \\alpha[X_{j_i}\\cdot (\\hat{Y}_i -Y_i)]& \\text{if } j > 0\\\\\n",
        "    \\end{array}\n",
        "\\right\\}\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "c_0AUKMGqihc"
      }
    }
  ]
}